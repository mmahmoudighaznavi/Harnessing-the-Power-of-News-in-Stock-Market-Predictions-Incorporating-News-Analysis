{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da45a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ta --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot\n",
    "from joblib import dump, load\n",
    "from pickle import load\n",
    "\n",
    "\n",
    "from keras import models\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "\n",
    "from pandas_datareader.data import DataReader\n",
    "from pandas_datareader import data as pdr\n",
    "import pandas_ta as ta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b99d30",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fe7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(ticker, start_date, end_date, interval):\n",
    "    \"\"\"\n",
    "    Fetch stock data for a given ticker, start date, and end date.\n",
    "    \n",
    "    :param ticker: Stock ticker symbol (e.g., 'AAPL' for Apple Inc.)\n",
    "    :param start_date: Start date in the format 'YYYY-MM-DD'.\n",
    "    :param end_date: End date in the format 'YYYY-MM-DD'.\n",
    "    \n",
    "    :return: DataFrame containing stock data for the given ticker and dates.\n",
    "    \"\"\"\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date, interval=interval)\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"AAPL\"\n",
    "start_date = \"2012-01-01\"\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "aapl_data = fetch_stock_data(ticker, start_date, end_date, interval = \"1d\")\n",
    "aapl_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9f19c",
   "metadata": {},
   "source": [
    "## Calculating Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c3762",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208e00d",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e727fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stat_df = aapl_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_stat_df1 = summary_stat_df.iloc[:,1:7]\n",
    "summary_stat_df1\n",
    "\n",
    "summary_stat = summary_stat_df1.describe()\n",
    "summary_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c4484",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "\n",
    "Visualize the distribution and identify skewness or kurtosis in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting all the histograms together\n",
    "for column, color in zip(['Open', 'High', 'Low', 'Close'], ['blue', 'green', 'red', 'purple']):\n",
    "    plt.hist(aapl_data[column], bins=50, alpha=0.5, label=column, color=color)\n",
    "\n",
    "plt.title('Price Distributions')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMA Moving Average\n",
    "sma_period = 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1314df2",
   "metadata": {},
   "source": [
    "The 200-day simple moving average (SMA) is considered a key indicator by traders and market analysts for determining overall long-term market trends. It is calculated by plotting the average price over the past 200 days, along with the daily price chart and other moving averages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c439129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
    "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
    "\n",
    "    average_gain = gain.rolling(window=period).mean()\n",
    "    average_loss = loss.rolling(window=period).mean()\n",
    "\n",
    "    rs = average_gain / average_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a094ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicator(aapl_data):\n",
    "    #Simple Moving Average (SMA)\n",
    "    aapl_data[\"SMA-200\"] = aapl_data.iloc[:,4].rolling(sma_period).mean()\n",
    "    \n",
    "    # Exponential Moving Average (EMA)\n",
    "    aapl_data[\"EMA-12\"] = aapl_data.iloc[:,4].ewm(span=12, adjust=False).mean()\n",
    "    aapl_data[\"EMA-26\"] = aapl_data.iloc[:,4].ewm(span=26, adjust=False).mean()\n",
    "    \n",
    "   #Average Convergence Divergence (MACD)\n",
    "    aapl_data['MACD'] = aapl_data[\"EMA-12\"] - aapl_data[\"EMA-26\"]\n",
    "    aapl_data['MACD_Signal'] = aapl_data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    aapl_data[\"RSI_14\"] = calculate_rsi(aapl_data['Close'], 14)\n",
    "    \n",
    "    return aapl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cf425",
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_data = get_indicator(aapl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_sma = px.line(appl_data, x=\"Date\", y=[\"Close\", \"SMA-200\", \"EMA-12\", \"EMA-26\"], title='Indicators')\n",
    "display(aapl_sma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "correlation_matrix = aapl_data.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the missing values\n",
    "aapl_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null values in the indicators columns\n",
    "aapl_data.dropna(subset=['SMA-200', 'EMA-12', 'EMA-26', 'MACD', 'MACD_Signal', 'RSI_14'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_data.to_csv(\"Stock_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95306f",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "If we use an 80/20 train/test split, we can determine the split data for training dataset from `2012-10-16 to 2021-09-30` and for testing dataset `2021-10-01 to 2023-11-08`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the data\n",
    "aapl_data = pd.read_csv(\"Stock_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_data.drop(columns=[\"Date\",\"Adj Close\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35137aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(aapl_data)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in aapl_data.columns if col != \"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bfc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_data(data, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(data) - n_steps):\n",
    "        seq_x, seq_y = data[i:i+n_steps][feature_columns], data.iloc[i+n_steps]['Close']\n",
    "        X.append(seq_x.values)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adea886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, n_steps, test_size):\n",
    "    # Create the windowed data\n",
    "    X, y = create_windowed_data(data, n_steps)\n",
    "    \n",
    "    # Calculate the index for the split point\n",
    "    test_samples = int(len(X) * test_size)\n",
    "    split_point = len(X) - test_samples\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X[:split_point], X[split_point:]\n",
    "    y_train, y_test = y[:split_point], y[split_point:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = data_split(aapl_data, n_steps, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e10b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73339a",
   "metadata": {},
   "source": [
    "### Scaling the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd591a",
   "metadata": {},
   "source": [
    "- [ ] Both X_train and X_test should be scaled. LSTM and other gradient-based algorithms used to transform both training and test data. \n",
    "\n",
    "- [ ] The scaler should be fitted only on the training data to avoid data leakage and then used to transform both the training data and the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5639cff",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "## Baseline GRU Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8d0b2",
   "metadata": {},
   "source": [
    "### Build GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b64c83",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eac4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPricePredictorGRU:\n",
    "    def __init__(self, n_steps, n_features):\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(GRU(units=50, return_sequences=True, input_shape=(self.n_steps, self.n_features), activation='tanh'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(units=50, activation='tanh'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(units=1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y, epochs=100, batch_size=32, validation_split=0.1):\n",
    "        # Reshape from 3D to 2D\n",
    "        nsamples, nx, ny = X.shape\n",
    "        X_2D = X.reshape((nsamples, nx*ny))\n",
    "\n",
    "        # Scale the data\n",
    "        X_scaled_2D = self.scaler.fit_transform(X_2D)\n",
    "\n",
    "        # Reshape back to 3D\n",
    "        X_scaled = X_scaled_2D.reshape((nsamples, nx, ny))\n",
    "\n",
    "        # Scale the target\n",
    "        y_scaled = self.y_scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "        # Fit the model\n",
    "        self.history = self.model.fit(\n",
    "            X_scaled, y_scaled,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
    "        )\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.history.history['loss'], 'bo-', label='Training loss')\n",
    "        plt.plot(self.history.history['val_loss'], 'ro-', label='Validation loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Reshape from 3D to 2D\n",
    "        nsamples, nx, ny = X.shape\n",
    "        X_2D = X.reshape((nsamples, nx*ny))\n",
    "        X_scaled_2D = self.scaler.transform(X_2D)\n",
    "        X_scaled = X_scaled_2D.reshape((nsamples, nx, ny))\n",
    "        y_pred_scaled = self.model.predict(X_scaled)\n",
    "        y_pred = self.y_scaler.inverse_transform(y_pred_scaled)\n",
    "        return y_pred\n",
    "\n",
    "    def plot_predictions(self, y_true, y_pred):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true, color='blue', label='Actual Stock Price')\n",
    "        plt.plot(y_pred, color='red', linestyle='--', label='Predicted Stock Price')\n",
    "        plt.title('Stock Price Prediction')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Stock Price')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluation(self, y_true, y_pred):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))) * 100\n",
    "        rmsde = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        \n",
    "        metrics = {'MAE': mae, 'MAPE': mape, 'R2': r2, 'RMSPE': rmspe, 'RMSDE': rmsde}\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.2f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "        \n",
    "    def save_scaler(self, scaler_path, y_scaler_path):\n",
    "        dump(self.scaler, scaler_path)\n",
    "        dump(self.y_scaler, y_scaler_path)\n",
    "    \n",
    "    def save_model(self, file_path):\n",
    "        self.model.save(file_path)\n",
    "        \n",
    "n_steps = 10\n",
    "n_features = 10\n",
    "\n",
    "predictor = StockPricePredictorGRU(n_steps=n_steps, n_features=n_features)\n",
    "predictor.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "predictor.plot_loss()\n",
    "y_pred = predictor.predict(X_test)\n",
    "\n",
    "predictor.plot_predictions(y_test, y_pred)\n",
    "\n",
    "metrics = predictor.evaluation(y_test, y_pred)\n",
    "\n",
    "predictor.save_model('Models/gru_model.h5')\n",
    "predictor.save_scaler('Scaler/X_scaler.joblib', 'Scaler/y_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d02254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load as joblib_load\n",
    "\n",
    "GRU_model = models.load_model('Models/gru_model.h5')\n",
    "X_scaler = open('Scaler/X_scaler.joblib', 'rb')\n",
    "y_scaler = open('Scaler/y_scaler.joblib', 'rb')\n",
    "\n",
    "X_scaler = joblib_load(X_scaler)\n",
    "y_scaler = joblib_load(y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ba42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_days(model, X_scaler, y_scaler, data, n_days, n_steps=10, n_features=10):\n",
    "    predictions = []\n",
    "    feature_columns = ['Open', 'High', 'Low', 'Volume', 'SMA-200', 'EMA-12', 'EMA-26', 'MACD', \n",
    "                       'MACD_Signal', 'RSI_14']\n",
    "\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    for _ in range(n_days):\n",
    "        input_data = data_copy[feature_columns].tail(n_steps)\n",
    "\n",
    "        # Flatten the data and reshape for the scaler to (1, n_steps * n_features)\n",
    "        input_data_flattened = input_data.values.flatten().reshape(1, n_steps * n_features)\n",
    "\n",
    "        # Scale the flattened data\n",
    "        input_data_scaled = X_scaler.transform(input_data_flattened)\n",
    "\n",
    "        # Reshape the scaled data back to 3D format for the model: (1, n_steps, n_features)\n",
    "        input_data_reshaped = input_data_scaled.reshape(1, n_steps, n_features)\n",
    "\n",
    "        predicted_price_scaled = model.predict(input_data_reshaped)\n",
    "\n",
    "        predicted_price = y_scaler.inverse_transform(predicted_price_scaled)[0, 0]\n",
    "\n",
    "        predictions.append(predicted_price)\n",
    "\n",
    "        next_row = input_data.iloc[-1].copy()\n",
    "        for col in feature_columns:\n",
    "            next_row[col] = input_data[col].mean()  \n",
    "        data_copy = pd.concat([data_copy, next_row], ignore_index=True)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "n_days = 1  # Number of days to predict\n",
    "predicted_prices = predict_next_days(GRU_model, X_scaler, y_scaler, aapl_data, n_days)\n",
    "predicted_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253482cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
